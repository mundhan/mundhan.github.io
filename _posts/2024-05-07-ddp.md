---
layout: single
title: "분산학습 관련 정리"
categories: model_training
tags: [deep learning, llm, huggingface, python]
---

* Question
    - vllm
        - lora_request 방식과 lora config를 모델에 Merge하는 방식의 차이?
        - ddp와 deepspeed 등 모델 최적화 및 분산학습 방법 각각의 장단점 및 사용상황
            - https://huggingface.co/docs/transformers/v4.35.0/ko/perf_train_gpu_many (참고, 허깅페이스 문서)
    - Dl 모델 전반
        - safetensors 및 pytorch_model.bin

* To do
    - deepspeed 적용
        - 딥스피드 문서
            - https://www.deepspeed.ai/getting-started/#resource-configuration-multi-node
            - https://www.deepspeed.ai/tutorials/
            - https://github.com/microsoft/DeepSpeed/blob/master/docs/_tutorials/inference-tutorial.md
            - https://wandb.ai/byyoung3/ml-news/reports/A-Guide-to-DeepSpeed-Zero-With-the-HuggingFace-Trainer--Vmlldzo2ODkwMDc4

        - Trainer - deepspeed Integration
            - https://huggingface.co/docs/transformers/v4.15.0/main_classes/deepspeed
        - step 1, 2는 인터넷 문서만 잘 따라가도 적용 용이. 여기까지만 해도 절감 많이 된다고 한다.
            - 각 스텝별 개요
                - https://moon-walker.medium.com/large-model-%ED%95%99%EC%8A%B5%EC%9D%98-game-changer-ms%EC%9D%98-deepspeed-zero-1-2-3-%EA%B7%B8%EB%A6%AC%EA%B3%A0-zero-infinity-74c9640190de
        - step 3의 경우, compute environment에 따라 잘 안되는 경우 왕왕 발생
        
* 분산학습
    - DP: data parallel
        - 하나의 디바이스가 데이터를 나누고, 각 디바이스에서의 처리 결과를 다시 모아 계산하므로 하나의 디바이스가 다른 디바이스에 비해 메모리 사용량이 많아짐 >>> 메모리 불균형 문제
    - DDP: distributed data parallel
        - 각 디바이스를 하나의 프로세스로 보고, 각 프로세스에서 모델을 띄어서 사용.
        (개행)역전파하는 과정에서만 내부적으로 그래디언트를 동기화하므로 불균형을 일으키지 않음.
            - "내부적으로 그래디언트를 동기화" 한다는 게 무슨 원리일까? 

* Docker
    - https://javacan.tistory.com/entry/docker-start-7-create-image-using-dockerfile
    - docker 작업 환경을 다른 pc와 맞출 때, requirements.txt로 설치하는 등의 방법으로는 시간이 오래 걸릴 수 있음.
    - >> 작업한 컨테이너를 이미지로 생성하고 생성한 이미지를 Tar로 저장해서 원하는 pc에서 load 해주기
        - container 2 image
            - docker commit 컨테이너명 이미지명:tag
        - image 2 tar
            - docker save -o 파일명.Tar 이미지명:Tag
        - tar 2 image
            - docker load -i 파일명.tar
    - 서버간 이동할 일 때문에 참고
        - scp 전송할 파일 상대서버계정@상대서버ip:상대서버디렉토리
    - docker image로 container 생성
        - '''python 
        docker create —-name container image /bin/bash
        '''
    - docker 컨테이너 실행 
        - '''python
docker run --runtime=nvidia \
--gpus all\ 
--name vllm_test\ 
—-entrypoint /bin/bash\
--env NVIDIA_DISABLE_REQUIRE=1\
-v /home/shhan/gpt_model\
-v ~/.cache/huggingface:/root/.cache/huggingface\ 
-it vllm_image:for_test

###
sudo docker run --runtime=nvidia --gpus all --name vllm_llama_test --entrypoint /bin/bash --env NVIDIA_DISABLE_REQUIRE=1 -v /home/shhan/gpt_model/ -v ~/.cache/huggingface:/root/.cache/huggingface -it vllm_image:for_test
        ''' 
    - docker 컨테이너 진입
        - '''python
        sudo docker exec -it e777ce59a3a0 /bin/bash
        '''
    - runtime=nvidia가 문제일 경우
        - nvidia-container-toolkit 및 nvidia-container-runtime 설치하고 재시작.
        - 내 경우는 toolkit만 설치 후 --runtime 옵션 빼고 돌림
    '''sudo docker run --gpus all --name vllm_llama_test --entrypoint /bin/bash --env NVIDIA_DISABLE_REQUIRE=1 -v /Data/home/shhan/vllm:/workspace/vllm -it vllm_image:for_test
    '''

* VScode 바로가기 
    - https://www.metacode9.com/entry/vscode-%ED%82%A4%EB%B3%B4%EB%93%9C-%EB%8B%A8%EC%B6%95%ED%82%A4shortcut-%EB%B0%94%EB%A1%9C%EA%B0%80%EA%B8%B0

* to do 2
    - llama3 lora 학습 후 merge
    - llama2 base 성능 node10에서 테스트
    - 

* Deepspeed
    - ddp의 --nproc_per_node >>> deepspeed의 --num_gpus
        - 텐서들의 생명 주기가 달라, 메모리 단편화 발생. 학습 시 
        연속적 메모리의 부족으로 가용 메모리가 남아 있음에도 불구하고 메모리 할당에 실패.
    - ZeRO-2는 메모리 단편화를 제거
    - sudo apt install libaio-dev
    -  [WARNING]  Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
        - 미해결
    - triton==1.0.0
        - https://github.com/microsoft/DeepSpeed/discussions/2408

    
            
---
layout: single
title: "GPT 모델의 -."
---

* llama는 2.7B가 아니라, llama2 7B
* Mistral도 같은 7B이지만 llama보다 일반적 성능이 높은 것으로 파악
  * 문법검사 접목 가능성 확인을 위해 Mistral로 다음 진행
    * fine-tuning
    * grammar correction

0. Data Preparation
<pre><code>
{"input": "What color is the sky?", "output": "The sky is blue."}
{"input": "Where is the best place to get cloud GPUs?", "output": "Brev.dev"}
</code></pre>

아래 코드로 위 형태와 같이 변환
<pre><code>
import pandas as pd
import json
import sys

csv_file = sys.argv[1]
csv_name = csv_file.replace(".csv","")

# CSV 파일 읽기
df = pd.read_csv(f'{csv_file}')

# 데이터 변환
with open(f'transformed_{csv_name}.jsonl', 'w') as jsonl_file:
    for index, row in df.iterrows():
        input_sentence = row['input'].replace('gec: ', '').strip()
        target_sentence = row['target']
        json_obj = {"input": input_sentence, "output": target_sentence}
        jsonl_file.write(json.dumps(json_obj) + '\n')

print(f"변환된 데이터가 transformed_{csv_name} 파일로 저장되었습니다.")  
</code></pre>


1. Mistral 7B model fine-tuning
   * WandB (Weights & Biases)
     * 특징: framework agnostic(PyTorch, Hugging Face 등), environment agnostic (AWS, Kubernetes 등)
   *
   *
   * 

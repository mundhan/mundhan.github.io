---
layout: single
title: "모델 최적화 관련 시도"
---

* onnx는 pytorch, tensorflow와 같은 딥러닝 프레임워크와 관계없이 단말 등에서 모델이 작동할 수 있도록 공통된 포맷 및 파라미터 형식을 지원하는 공유 플랫폼
  * 그래서 pytorch 한번 onnx로 바뀐 뒤에 fine-tuning 등의 추가 학습은 어렵다. 모델 형식이 바뀌어 기존 프레임워크에서 제공되는 기능을 사용할 수 없기 때문. 
  * 그래서 보통 잘 훈련된 모델을 Porting하는 용도로 사용한다.
 
* 최근에 시도한 모델 최적화 방법 목록
  * fastT5
    * 기존 모델, onnx 변환 모델, onnx 변환 및 양자화 모델을 cpu 환경(fastT5에서 사용하는 onnxruntime이 gpu 지원 안 함)에서 돌려본 결과 기존 모델이 2-3배 가량 빠름
    * onnx 설정 상 문제가 있을지 의심되지만 그런 정보 확인하지 못함
    * 공정한 computation 조건에서 진행되었는지 확인 필요 (e.g. cpu core를 몇대 사용하였는지)
      * cpu single core 기준으로 확인 (코어당 1 프로세스 개념)
      * 멀티 코어일수록 스루풋은 낮아진다
      * 단말 코어는 2-4개로 low-resource 환경을 가정한다.
  * fasterTransformer
    * C++ 기반
    * 적용 방법에 대한 레퍼런스가 적어, 예제 코드와 유사하게 코드를 수정하였으나 현재 난항 중

* 메모리 효율을 높이고 속도를 향상 시키기 위한 방법으로 기존 T5 모델을 GPT 모델(e.g. llama)로 대체할 수 있을지 조사가 필요 


### tensorrt-llm
#### 개요

###### cuda version으로 docker run이 안 될 경우
<pre>
<code>
sudo docker run --rm --runtime=nvidia --gpus all --entrypoint /bin/bash --env NVIDIA_DISABLE_REQUIRE=1 -it nvidia/cuda:12.1.0-devel-ubuntu22.04
</code>
</pre>
-- rm: 일회성 옵션 (컨테이너 종료 시 컨테이너 삭제)
-v: volume 마운트 (-v /Data/home/shhan/test:/workspace)
다시 접속할때는, docker start [컨테이너id] 후, docker exec -it 3b076a4ebe95 /bin/bash


###### tensorrt_llm을 pip install 하는 것이 불가할 경우
<pre>
<code>
pip3 install --extra-index-url https://pypi.nvidia.com/ tensorrt_llm==0.9.0.dev2024040200
</code>
</pre>

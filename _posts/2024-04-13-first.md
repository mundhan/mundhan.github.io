---
layout: single
title: "FasterTransformer 적용 방법 조사"
categories: coding
tags: [opt, python, dl, test]
---

# 
FasterTransformer는 인코더 및 디코더의 추론(inference)를 위한 트랜스포머 레이어를 구현

현재 내가 가지고 있는 모델의 torch.dtype은 fp32

```python
def train(
    # model/data params
    base_model: str = "",  # the only required argument
    data_path: str = "",   
    output_dir: str = "",
    # training hyperparams
    batch_size: int = 4,
    micro_batch_size: int = 1,
    num_epochs: int = 2,
    learning_rate: float = 3e-4,
    cutoff_len: int = 128,
    val_set_size: int = 20,
    # llm hyperparams
    train_on_inputs: bool = True,  # if False, masks out inputs in loss
    group_by_length: bool = False,  # faster, but produces an odd training loss curve
    # wandb params
    wandb_project: str = "",
    wandb_run_name: str = "",
    wandb_watch: str = "",  # options: false | gradients | all
    wandb_log_model: str = "",  # options: false | true
    resume_from_checkpoint: str = "",  # either training checkpoint or final adapter
    prompt_template_name: str = "grammar",  # The prompt template to use, will default to alpaca.
):
    if int(os.environ.get("LOCAL_RANK", 0)) == 0:
        print(
            f"Training Alpaca-LoRA model with params:\n"
            f"base_model: {base_model}\n"
            f"data_path: {data_path}\n"
            f"output_dir: {output_dir}\n"
            f"batch_size: {batch_size}\n"
            f"micro_batch_size: {micro_batch_size}\n"
            f"num_epochs: {num_epochs}\n"
            f"learning_rate: {learning_rate}\n"
            f"cutoff_len: {cutoff_len}\n"
            f"val_set_size: {val_set_size}\n"
            f"train_on_inputs: {train_on_inputs}\n"
            f"group_by_length: {group_by_length}\n"
            f"resume_from_checkpoint: {resume_from_checkpoint or False}\n"
            f"prompt template: {prompt_template_name}\n"
        )
```

![스크린샷 2024-03-25 오전 10.54.44](../images/2024-04-13-first/스크린샷 2024-03-25 오전 10.54.44.png)
